{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. GAN\n",
    "### reference \n",
    "- https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py\n",
    "- https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice\n",
    "- paper : https://arxiv.org/abs/1406.2661"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting easydict\n",
      "  Downloading easydict-1.9.tar.gz (6.4 kB)\n",
      "Building wheels for collected packages: easydict\n",
      "  Building wheel for easydict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for easydict: filename=easydict-1.9-py3-none-any.whl size=6350 sha256=a1fc36f5f06448f8be14a872704b7bfd19a590807c6b70b4e69e985d5bbfa979\n",
      "  Stored in directory: /root/.cache/pip/wheels/88/96/68/c2be18e7406804be2e593e1c37845f2dd20ac2ce1381ce40b0\n",
      "Successfully built easydict\n",
      "Installing collected packages: easydict\n",
      "Successfully installed easydict-1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse # -> jupyter notebook X\n",
    "import easydict\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make images folder\n",
    "os.makedirs('images', exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = easydict.EasyDict({\"n_epochs\" : 5,\n",
    "                         \"batch_size\" : 64,\n",
    "                         \"lr\" : 0.0002,\n",
    "                         \"b1\" : 0.5,\n",
    "                         \"b2\" : 0.99,\n",
    "                         \"n_cpu\" : 8,\n",
    "                         \"latent_dim\" : 100,\n",
    "                         \"img_size\" : 28,\n",
    "                         \"channels\" : 1,\n",
    "                         \"sample_interval\" : 400})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_epochs': 200, 'batch_size': 64, 'lr': 0.0002, 'b1': 0.5, 'b2': 0.99, 'n_cpu': 8, 'latent_dim': 100, 'img_size': 28, 'channels': 1, 'sample_interval': 400}\n"
     ]
    }
   ],
   "source": [
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (opt.channels, opt.img_size, opt.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Learnable Parameters\n",
    "m = nn.BatchNorm1d(100)\n",
    "# Without Learnable Parameters\n",
    "m = nn.BatchNorm1d(100, affine=False)\n",
    "input = torch.randn(20, 100)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 100])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Linear -> batchNorm (on, off) -> LeakyReLU \n",
    "        def block(in_feat, out_feat, normalize = True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            # mini batch normalization\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            # slope : 0.2 -> if x is negative x * 0.2\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace = True))\n",
    "            return layers\n",
    "        \n",
    "        # img_shape : 1 * 28 * 28, prod(img_shape) : 784\n",
    "        # tanh : exp(x) - exp(-x) / exp(x) + exp(-x) , range : (-1, 1)\n",
    "        self.model = nn.Sequential(*block(opt.latent_dim, 128, normalize = False),\n",
    "                                   *block(128, 256),\n",
    "                                   *block(256, 512),\n",
    "                                   *block(512, 1024),\n",
    "                                   nn.Linear(1024, int(np.prod(img_shape))),\n",
    "                                   nn.Tanh()\n",
    "                                  )\n",
    "    # z is random noise\n",
    "    # z size : batch * latent size\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        # img size : batch * prod(img_shape)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # probability result\n",
    "        self.model = nn.Sequential(nn.Linear(int(np.prod(img_shape)), 512),\n",
    "                                   nn.LeakyReLU(0.2, inplace = True),\n",
    "                                   nn.Linear(512, 256),\n",
    "                                   nn.LeakyReLU(0.2, inplace = True),\n",
    "                                   nn.Linear(256, 1),\n",
    "                                   nn.Sigmoid()\n",
    "                                  )\n",
    "    \n",
    "    # img size : batch size * channel * height * width\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross Entropy Loss\n",
    "adversarial_loss = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Loader and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data Loader (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-22 13:48:39--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
      "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
      "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
      "--2021-04-22 13:48:40--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
      "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-gzip]\n",
      "Saving to: ‘MNIST.tar.gz’\n",
      "\n",
      "MNIST.tar.gz            [             <=>    ]  33.20M  12.8MB/s    in 2.6s    \n",
      "\n",
      "2021-04-22 13:48:43 (12.8 MB/s) - ‘MNIST.tar.gz’ saved [34813078]\n",
      "\n",
      "MNIST/\n",
      "MNIST/raw/\n",
      "MNIST/raw/train-labels-idx1-ubyte\n",
      "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "MNIST/raw/t10k-labels-idx1-ubyte\n",
      "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "MNIST/raw/train-images-idx3-ubyte\n",
      "MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "MNIST/raw/t10k-images-idx3-ubyte\n",
      "MNIST/raw/train-images-idx3-ubyte.gz\n",
      "MNIST/processed/\n",
      "MNIST/processed/training.pt\n",
      "MNIST/processed/test.pt\n"
     ]
    }
   ],
   "source": [
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "!tar -zxvf MNIST.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "os.makedirs(\"./\", exist_ok = True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "                datasets.MNIST(\n",
    "                    \"./\",\n",
    "                    train = True,\n",
    "                    download = True,\n",
    "                    transform = transforms.Compose(\n",
    "                        [transforms.Resize(opt.img_size), transforms.ToTensor(),\n",
    "                         transforms.Normalize([0.5], [0.5])]\n",
    "                    )\n",
    "                ),\n",
    "                batch_size = opt.batch_size,\n",
    "                shuffle = True\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr = opt.lr, betas = (opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr = opt.lr, betas = (opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 5] [Batch 0 / 938] [D loss: 0.708330] [G loss: 0.685343]\n",
      "[Epoch 0 / 5] [Batch 20 / 938] [D loss: 0.416714] [G loss: 0.616385]\n",
      "[Epoch 0 / 5] [Batch 40 / 938] [D loss: 0.492486] [G loss: 0.606595]\n",
      "[Epoch 0 / 5] [Batch 60 / 938] [D loss: 0.622108] [G loss: 0.453586]\n",
      "[Epoch 0 / 5] [Batch 80 / 938] [D loss: 0.458567] [G loss: 0.820725]\n",
      "[Epoch 0 / 5] [Batch 100 / 938] [D loss: 0.479186] [G loss: 0.709180]\n",
      "[Epoch 0 / 5] [Batch 120 / 938] [D loss: 0.455426] [G loss: 0.811841]\n",
      "[Epoch 0 / 5] [Batch 140 / 938] [D loss: 0.377666] [G loss: 1.227942]\n",
      "[Epoch 0 / 5] [Batch 160 / 938] [D loss: 0.463690] [G loss: 0.772292]\n",
      "[Epoch 0 / 5] [Batch 180 / 938] [D loss: 0.705680] [G loss: 0.858457]\n",
      "[Epoch 0 / 5] [Batch 200 / 938] [D loss: 0.613616] [G loss: 0.564179]\n",
      "[Epoch 0 / 5] [Batch 220 / 938] [D loss: 0.513266] [G loss: 1.158929]\n",
      "[Epoch 0 / 5] [Batch 240 / 938] [D loss: 0.675653] [G loss: 0.439165]\n",
      "[Epoch 0 / 5] [Batch 260 / 938] [D loss: 0.516442] [G loss: 0.680341]\n",
      "[Epoch 0 / 5] [Batch 280 / 938] [D loss: 0.372583] [G loss: 1.039778]\n",
      "[Epoch 0 / 5] [Batch 300 / 938] [D loss: 0.565404] [G loss: 1.024405]\n",
      "[Epoch 0 / 5] [Batch 320 / 938] [D loss: 0.577547] [G loss: 0.940454]\n",
      "[Epoch 0 / 5] [Batch 340 / 938] [D loss: 0.584512] [G loss: 1.020935]\n",
      "[Epoch 0 / 5] [Batch 360 / 938] [D loss: 0.520389] [G loss: 0.989783]\n",
      "[Epoch 0 / 5] [Batch 380 / 938] [D loss: 0.534434] [G loss: 0.694708]\n",
      "[Epoch 0 / 5] [Batch 400 / 938] [D loss: 0.515166] [G loss: 1.352350]\n",
      "[Epoch 0 / 5] [Batch 420 / 938] [D loss: 0.440344] [G loss: 1.375648]\n",
      "[Epoch 0 / 5] [Batch 440 / 938] [D loss: 0.368238] [G loss: 1.159915]\n",
      "[Epoch 0 / 5] [Batch 460 / 938] [D loss: 0.534052] [G loss: 0.599042]\n",
      "[Epoch 0 / 5] [Batch 480 / 938] [D loss: 0.478997] [G loss: 1.042971]\n",
      "[Epoch 0 / 5] [Batch 500 / 938] [D loss: 0.571616] [G loss: 0.726310]\n",
      "[Epoch 0 / 5] [Batch 520 / 938] [D loss: 0.514986] [G loss: 0.939450]\n",
      "[Epoch 0 / 5] [Batch 540 / 938] [D loss: 0.518247] [G loss: 1.059316]\n",
      "[Epoch 0 / 5] [Batch 560 / 938] [D loss: 0.468963] [G loss: 1.106120]\n",
      "[Epoch 0 / 5] [Batch 580 / 938] [D loss: 0.446386] [G loss: 1.211137]\n",
      "[Epoch 0 / 5] [Batch 600 / 938] [D loss: 0.432115] [G loss: 0.688546]\n",
      "[Epoch 0 / 5] [Batch 620 / 938] [D loss: 0.583202] [G loss: 2.119340]\n",
      "[Epoch 0 / 5] [Batch 640 / 938] [D loss: 0.484822] [G loss: 1.597934]\n",
      "[Epoch 0 / 5] [Batch 660 / 938] [D loss: 0.422960] [G loss: 0.927699]\n",
      "[Epoch 0 / 5] [Batch 680 / 938] [D loss: 0.429123] [G loss: 1.119392]\n",
      "[Epoch 0 / 5] [Batch 700 / 938] [D loss: 0.479012] [G loss: 0.870944]\n",
      "[Epoch 0 / 5] [Batch 720 / 938] [D loss: 0.430390] [G loss: 0.875358]\n",
      "[Epoch 0 / 5] [Batch 740 / 938] [D loss: 0.469290] [G loss: 1.128337]\n",
      "[Epoch 0 / 5] [Batch 760 / 938] [D loss: 0.454577] [G loss: 0.849474]\n",
      "[Epoch 0 / 5] [Batch 780 / 938] [D loss: 0.521352] [G loss: 0.557908]\n",
      "[Epoch 0 / 5] [Batch 800 / 938] [D loss: 0.709622] [G loss: 0.312705]\n",
      "[Epoch 0 / 5] [Batch 820 / 938] [D loss: 0.441781] [G loss: 0.850856]\n",
      "[Epoch 0 / 5] [Batch 840 / 938] [D loss: 0.763366] [G loss: 0.284595]\n",
      "[Epoch 0 / 5] [Batch 860 / 938] [D loss: 0.539727] [G loss: 1.705188]\n",
      "[Epoch 0 / 5] [Batch 880 / 938] [D loss: 0.529890] [G loss: 0.506937]\n",
      "[Epoch 0 / 5] [Batch 900 / 938] [D loss: 0.611699] [G loss: 2.008152]\n",
      "[Epoch 0 / 5] [Batch 920 / 938] [D loss: 0.586753] [G loss: 0.504475]\n",
      "[Epoch 1 / 5] [Batch 0 / 938] [D loss: 0.465853] [G loss: 0.979294]\n",
      "[Epoch 1 / 5] [Batch 20 / 938] [D loss: 0.446142] [G loss: 1.033619]\n",
      "[Epoch 1 / 5] [Batch 40 / 938] [D loss: 0.437253] [G loss: 0.975037]\n",
      "[Epoch 1 / 5] [Batch 60 / 938] [D loss: 0.636021] [G loss: 0.421149]\n",
      "[Epoch 1 / 5] [Batch 80 / 938] [D loss: 0.406699] [G loss: 1.531467]\n",
      "[Epoch 1 / 5] [Batch 100 / 938] [D loss: 0.498683] [G loss: 1.126306]\n",
      "[Epoch 1 / 5] [Batch 120 / 938] [D loss: 0.656338] [G loss: 2.403139]\n",
      "[Epoch 1 / 5] [Batch 140 / 938] [D loss: 0.513180] [G loss: 0.741764]\n",
      "[Epoch 1 / 5] [Batch 160 / 938] [D loss: 0.771943] [G loss: 0.295313]\n",
      "[Epoch 1 / 5] [Batch 180 / 938] [D loss: 0.373618] [G loss: 0.947844]\n",
      "[Epoch 1 / 5] [Batch 200 / 938] [D loss: 0.553461] [G loss: 0.468735]\n",
      "[Epoch 1 / 5] [Batch 220 / 938] [D loss: 0.487134] [G loss: 1.082887]\n",
      "[Epoch 1 / 5] [Batch 240 / 938] [D loss: 0.444904] [G loss: 2.211803]\n",
      "[Epoch 1 / 5] [Batch 260 / 938] [D loss: 0.367238] [G loss: 1.777364]\n",
      "[Epoch 1 / 5] [Batch 280 / 938] [D loss: 0.334958] [G loss: 1.077635]\n",
      "[Epoch 1 / 5] [Batch 300 / 938] [D loss: 0.407359] [G loss: 1.392840]\n",
      "[Epoch 1 / 5] [Batch 320 / 938] [D loss: 0.504201] [G loss: 0.627021]\n",
      "[Epoch 1 / 5] [Batch 340 / 938] [D loss: 0.375596] [G loss: 0.876978]\n",
      "[Epoch 1 / 5] [Batch 360 / 938] [D loss: 0.470942] [G loss: 0.664086]\n",
      "[Epoch 1 / 5] [Batch 380 / 938] [D loss: 0.395689] [G loss: 0.826441]\n",
      "[Epoch 1 / 5] [Batch 400 / 938] [D loss: 0.421542] [G loss: 1.185942]\n",
      "[Epoch 1 / 5] [Batch 420 / 938] [D loss: 0.624647] [G loss: 3.189133]\n",
      "[Epoch 1 / 5] [Batch 440 / 938] [D loss: 0.335349] [G loss: 0.978796]\n",
      "[Epoch 1 / 5] [Batch 460 / 938] [D loss: 0.294629] [G loss: 1.491985]\n",
      "[Epoch 1 / 5] [Batch 480 / 938] [D loss: 0.652331] [G loss: 0.397793]\n",
      "[Epoch 1 / 5] [Batch 500 / 938] [D loss: 0.503981] [G loss: 0.585125]\n",
      "[Epoch 1 / 5] [Batch 520 / 938] [D loss: 0.289087] [G loss: 1.116305]\n",
      "[Epoch 1 / 5] [Batch 540 / 938] [D loss: 0.311247] [G loss: 2.221816]\n",
      "[Epoch 1 / 5] [Batch 560 / 938] [D loss: 0.351324] [G loss: 1.122416]\n",
      "[Epoch 1 / 5] [Batch 580 / 938] [D loss: 0.340417] [G loss: 0.964958]\n",
      "[Epoch 1 / 5] [Batch 600 / 938] [D loss: 0.391030] [G loss: 0.862696]\n",
      "[Epoch 1 / 5] [Batch 620 / 938] [D loss: 0.349803] [G loss: 1.801151]\n",
      "[Epoch 1 / 5] [Batch 640 / 938] [D loss: 0.554026] [G loss: 0.432267]\n",
      "[Epoch 1 / 5] [Batch 660 / 938] [D loss: 0.418663] [G loss: 0.841905]\n",
      "[Epoch 1 / 5] [Batch 680 / 938] [D loss: 0.370538] [G loss: 1.018832]\n",
      "[Epoch 1 / 5] [Batch 700 / 938] [D loss: 0.817647] [G loss: 0.262860]\n",
      "[Epoch 1 / 5] [Batch 720 / 938] [D loss: 0.947287] [G loss: 0.194713]\n",
      "[Epoch 1 / 5] [Batch 740 / 938] [D loss: 0.634211] [G loss: 0.384227]\n",
      "[Epoch 1 / 5] [Batch 760 / 938] [D loss: 0.403019] [G loss: 0.742398]\n",
      "[Epoch 1 / 5] [Batch 780 / 938] [D loss: 0.455486] [G loss: 0.641837]\n",
      "[Epoch 1 / 5] [Batch 800 / 938] [D loss: 0.467089] [G loss: 3.132786]\n",
      "[Epoch 1 / 5] [Batch 820 / 938] [D loss: 0.771185] [G loss: 2.891270]\n",
      "[Epoch 1 / 5] [Batch 840 / 938] [D loss: 0.393411] [G loss: 0.764873]\n",
      "[Epoch 1 / 5] [Batch 860 / 938] [D loss: 0.646407] [G loss: 0.414208]\n",
      "[Epoch 1 / 5] [Batch 880 / 938] [D loss: 0.295973] [G loss: 1.605415]\n",
      "[Epoch 1 / 5] [Batch 900 / 938] [D loss: 0.275182] [G loss: 1.229995]\n",
      "[Epoch 1 / 5] [Batch 920 / 938] [D loss: 0.306256] [G loss: 0.990727]\n",
      "[Epoch 2 / 5] [Batch 0 / 938] [D loss: 0.678361] [G loss: 0.377364]\n",
      "[Epoch 2 / 5] [Batch 20 / 938] [D loss: 0.405380] [G loss: 1.737826]\n",
      "[Epoch 2 / 5] [Batch 40 / 938] [D loss: 0.452973] [G loss: 1.621806]\n",
      "[Epoch 2 / 5] [Batch 60 / 938] [D loss: 0.420367] [G loss: 1.167428]\n",
      "[Epoch 2 / 5] [Batch 80 / 938] [D loss: 0.320786] [G loss: 1.158005]\n",
      "[Epoch 2 / 5] [Batch 100 / 938] [D loss: 0.398553] [G loss: 0.844372]\n",
      "[Epoch 2 / 5] [Batch 120 / 938] [D loss: 0.375637] [G loss: 1.037355]\n",
      "[Epoch 2 / 5] [Batch 140 / 938] [D loss: 0.626796] [G loss: 0.429168]\n",
      "[Epoch 2 / 5] [Batch 160 / 938] [D loss: 0.469753] [G loss: 0.820727]\n",
      "[Epoch 2 / 5] [Batch 180 / 938] [D loss: 0.509355] [G loss: 1.631286]\n",
      "[Epoch 2 / 5] [Batch 200 / 938] [D loss: 0.592978] [G loss: 0.466450]\n",
      "[Epoch 2 / 5] [Batch 220 / 938] [D loss: 0.286233] [G loss: 1.618364]\n",
      "[Epoch 2 / 5] [Batch 240 / 938] [D loss: 0.508553] [G loss: 0.532670]\n",
      "[Epoch 2 / 5] [Batch 260 / 938] [D loss: 0.744051] [G loss: 0.288198]\n",
      "[Epoch 2 / 5] [Batch 280 / 938] [D loss: 0.457578] [G loss: 3.124247]\n",
      "[Epoch 2 / 5] [Batch 300 / 938] [D loss: 0.894143] [G loss: 3.612283]\n",
      "[Epoch 2 / 5] [Batch 320 / 938] [D loss: 0.501780] [G loss: 0.518780]\n",
      "[Epoch 2 / 5] [Batch 340 / 938] [D loss: 0.438833] [G loss: 0.933923]\n",
      "[Epoch 2 / 5] [Batch 360 / 938] [D loss: 0.312799] [G loss: 2.151221]\n",
      "[Epoch 2 / 5] [Batch 380 / 938] [D loss: 0.179353] [G loss: 1.970367]\n",
      "[Epoch 2 / 5] [Batch 400 / 938] [D loss: 0.249442] [G loss: 1.336904]\n",
      "[Epoch 2 / 5] [Batch 420 / 938] [D loss: 0.216193] [G loss: 2.518353]\n",
      "[Epoch 2 / 5] [Batch 440 / 938] [D loss: 0.266821] [G loss: 3.033499]\n",
      "[Epoch 2 / 5] [Batch 460 / 938] [D loss: 0.805345] [G loss: 3.289331]\n",
      "[Epoch 2 / 5] [Batch 480 / 938] [D loss: 0.381211] [G loss: 2.027222]\n",
      "[Epoch 2 / 5] [Batch 500 / 938] [D loss: 0.358189] [G loss: 1.009081]\n",
      "[Epoch 2 / 5] [Batch 520 / 938] [D loss: 0.303290] [G loss: 1.904840]\n",
      "[Epoch 2 / 5] [Batch 540 / 938] [D loss: 0.369271] [G loss: 0.862393]\n",
      "[Epoch 2 / 5] [Batch 560 / 938] [D loss: 0.358739] [G loss: 1.013966]\n",
      "[Epoch 2 / 5] [Batch 580 / 938] [D loss: 0.390984] [G loss: 0.923648]\n",
      "[Epoch 2 / 5] [Batch 600 / 938] [D loss: 0.484443] [G loss: 2.875974]\n",
      "[Epoch 2 / 5] [Batch 620 / 938] [D loss: 0.156310] [G loss: 2.963507]\n",
      "[Epoch 2 / 5] [Batch 640 / 938] [D loss: 0.971764] [G loss: 0.181974]\n",
      "[Epoch 2 / 5] [Batch 660 / 938] [D loss: 0.347472] [G loss: 0.991372]\n",
      "[Epoch 2 / 5] [Batch 680 / 938] [D loss: 0.326588] [G loss: 2.156780]\n",
      "[Epoch 2 / 5] [Batch 700 / 938] [D loss: 0.306097] [G loss: 1.020402]\n",
      "[Epoch 2 / 5] [Batch 720 / 938] [D loss: 0.268247] [G loss: 1.247337]\n",
      "[Epoch 2 / 5] [Batch 740 / 938] [D loss: 0.284329] [G loss: 1.377003]\n",
      "[Epoch 2 / 5] [Batch 760 / 938] [D loss: 0.316520] [G loss: 0.895605]\n",
      "[Epoch 2 / 5] [Batch 780 / 938] [D loss: 0.489266] [G loss: 0.733298]\n",
      "[Epoch 2 / 5] [Batch 800 / 938] [D loss: 0.343515] [G loss: 0.889043]\n",
      "[Epoch 2 / 5] [Batch 820 / 938] [D loss: 0.510376] [G loss: 3.219285]\n",
      "[Epoch 2 / 5] [Batch 840 / 938] [D loss: 0.262187] [G loss: 1.788681]\n",
      "[Epoch 2 / 5] [Batch 860 / 938] [D loss: 0.345845] [G loss: 2.818876]\n",
      "[Epoch 2 / 5] [Batch 880 / 938] [D loss: 0.321184] [G loss: 1.148219]\n",
      "[Epoch 2 / 5] [Batch 900 / 938] [D loss: 0.278045] [G loss: 3.130224]\n",
      "[Epoch 2 / 5] [Batch 920 / 938] [D loss: 0.558221] [G loss: 0.449018]\n",
      "[Epoch 3 / 5] [Batch 0 / 938] [D loss: 0.443406] [G loss: 2.915950]\n",
      "[Epoch 3 / 5] [Batch 20 / 938] [D loss: 0.271379] [G loss: 1.137196]\n",
      "[Epoch 3 / 5] [Batch 40 / 938] [D loss: 0.469150] [G loss: 3.379012]\n",
      "[Epoch 3 / 5] [Batch 60 / 938] [D loss: 0.248065] [G loss: 2.088911]\n",
      "[Epoch 3 / 5] [Batch 80 / 938] [D loss: 0.235804] [G loss: 1.675540]\n",
      "[Epoch 3 / 5] [Batch 100 / 938] [D loss: 0.342918] [G loss: 2.144185]\n",
      "[Epoch 3 / 5] [Batch 120 / 938] [D loss: 0.364076] [G loss: 1.084677]\n",
      "[Epoch 3 / 5] [Batch 140 / 938] [D loss: 0.361727] [G loss: 2.164077]\n",
      "[Epoch 3 / 5] [Batch 160 / 938] [D loss: 0.254097] [G loss: 1.062115]\n",
      "[Epoch 3 / 5] [Batch 180 / 938] [D loss: 0.441325] [G loss: 3.266214]\n",
      "[Epoch 3 / 5] [Batch 200 / 938] [D loss: 0.489639] [G loss: 0.573106]\n",
      "[Epoch 3 / 5] [Batch 220 / 938] [D loss: 0.283752] [G loss: 1.192566]\n",
      "[Epoch 3 / 5] [Batch 240 / 938] [D loss: 0.312793] [G loss: 1.133113]\n",
      "[Epoch 3 / 5] [Batch 260 / 938] [D loss: 0.242167] [G loss: 2.721231]\n",
      "[Epoch 3 / 5] [Batch 280 / 938] [D loss: 0.427137] [G loss: 2.962646]\n",
      "[Epoch 3 / 5] [Batch 300 / 938] [D loss: 0.477921] [G loss: 3.195375]\n",
      "[Epoch 3 / 5] [Batch 320 / 938] [D loss: 0.349270] [G loss: 1.128733]\n",
      "[Epoch 3 / 5] [Batch 340 / 938] [D loss: 0.454339] [G loss: 2.408062]\n",
      "[Epoch 3 / 5] [Batch 360 / 938] [D loss: 0.518871] [G loss: 0.553256]\n",
      "[Epoch 3 / 5] [Batch 380 / 938] [D loss: 0.485943] [G loss: 0.888697]\n",
      "[Epoch 3 / 5] [Batch 400 / 938] [D loss: 0.249865] [G loss: 1.717451]\n",
      "[Epoch 3 / 5] [Batch 420 / 938] [D loss: 0.303089] [G loss: 1.118671]\n",
      "[Epoch 3 / 5] [Batch 440 / 938] [D loss: 0.430903] [G loss: 0.635092]\n",
      "[Epoch 3 / 5] [Batch 460 / 938] [D loss: 0.298326] [G loss: 1.658274]\n",
      "[Epoch 3 / 5] [Batch 480 / 938] [D loss: 0.387325] [G loss: 0.867533]\n",
      "[Epoch 3 / 5] [Batch 500 / 938] [D loss: 0.488990] [G loss: 1.128721]\n",
      "[Epoch 3 / 5] [Batch 520 / 938] [D loss: 0.379709] [G loss: 1.651256]\n",
      "[Epoch 3 / 5] [Batch 540 / 938] [D loss: 0.371504] [G loss: 0.855690]\n",
      "[Epoch 3 / 5] [Batch 560 / 938] [D loss: 0.539955] [G loss: 0.495088]\n",
      "[Epoch 3 / 5] [Batch 580 / 938] [D loss: 0.344522] [G loss: 1.271250]\n",
      "[Epoch 3 / 5] [Batch 600 / 938] [D loss: 0.261962] [G loss: 1.618357]\n",
      "[Epoch 3 / 5] [Batch 620 / 938] [D loss: 0.205974] [G loss: 1.341899]\n",
      "[Epoch 3 / 5] [Batch 640 / 938] [D loss: 0.327680] [G loss: 2.631555]\n",
      "[Epoch 3 / 5] [Batch 660 / 938] [D loss: 0.648416] [G loss: 0.399457]\n",
      "[Epoch 3 / 5] [Batch 680 / 938] [D loss: 0.361693] [G loss: 2.352927]\n",
      "[Epoch 3 / 5] [Batch 700 / 938] [D loss: 0.292482] [G loss: 1.440373]\n",
      "[Epoch 3 / 5] [Batch 720 / 938] [D loss: 0.233505] [G loss: 2.453286]\n",
      "[Epoch 3 / 5] [Batch 740 / 938] [D loss: 0.387873] [G loss: 1.092184]\n",
      "[Epoch 3 / 5] [Batch 760 / 938] [D loss: 0.412688] [G loss: 0.959132]\n",
      "[Epoch 3 / 5] [Batch 780 / 938] [D loss: 0.355419] [G loss: 1.145426]\n",
      "[Epoch 3 / 5] [Batch 800 / 938] [D loss: 0.288923] [G loss: 1.714750]\n",
      "[Epoch 3 / 5] [Batch 820 / 938] [D loss: 0.287808] [G loss: 1.748288]\n",
      "[Epoch 3 / 5] [Batch 840 / 938] [D loss: 0.334127] [G loss: 1.787303]\n",
      "[Epoch 3 / 5] [Batch 860 / 938] [D loss: 0.262899] [G loss: 1.460882]\n",
      "[Epoch 3 / 5] [Batch 880 / 938] [D loss: 0.260829] [G loss: 1.207232]\n",
      "[Epoch 3 / 5] [Batch 900 / 938] [D loss: 0.837637] [G loss: 5.435209]\n",
      "[Epoch 3 / 5] [Batch 920 / 938] [D loss: 0.502762] [G loss: 2.358144]\n",
      "[Epoch 4 / 5] [Batch 0 / 938] [D loss: 0.595610] [G loss: 0.479223]\n",
      "[Epoch 4 / 5] [Batch 20 / 938] [D loss: 0.489528] [G loss: 0.679270]\n",
      "[Epoch 4 / 5] [Batch 40 / 938] [D loss: 0.231438] [G loss: 1.468110]\n",
      "[Epoch 4 / 5] [Batch 60 / 938] [D loss: 0.462667] [G loss: 0.732445]\n",
      "[Epoch 4 / 5] [Batch 80 / 938] [D loss: 0.230495] [G loss: 2.433804]\n",
      "[Epoch 4 / 5] [Batch 100 / 938] [D loss: 0.185819] [G loss: 1.570934]\n",
      "[Epoch 4 / 5] [Batch 120 / 938] [D loss: 0.325719] [G loss: 0.903112]\n",
      "[Epoch 4 / 5] [Batch 140 / 938] [D loss: 0.301412] [G loss: 1.549008]\n",
      "[Epoch 4 / 5] [Batch 160 / 938] [D loss: 0.288300] [G loss: 3.280688]\n",
      "[Epoch 4 / 5] [Batch 180 / 938] [D loss: 0.437114] [G loss: 1.564380]\n",
      "[Epoch 4 / 5] [Batch 200 / 938] [D loss: 0.338554] [G loss: 1.163484]\n",
      "[Epoch 4 / 5] [Batch 220 / 938] [D loss: 0.263563] [G loss: 1.633937]\n",
      "[Epoch 4 / 5] [Batch 240 / 938] [D loss: 0.302959] [G loss: 1.291017]\n",
      "[Epoch 4 / 5] [Batch 260 / 938] [D loss: 0.374616] [G loss: 1.724831]\n",
      "[Epoch 4 / 5] [Batch 280 / 938] [D loss: 0.376882] [G loss: 0.818341]\n",
      "[Epoch 4 / 5] [Batch 300 / 938] [D loss: 0.374977] [G loss: 1.593013]\n",
      "[Epoch 4 / 5] [Batch 320 / 938] [D loss: 0.350026] [G loss: 0.886637]\n",
      "[Epoch 4 / 5] [Batch 340 / 938] [D loss: 0.207130] [G loss: 2.432507]\n",
      "[Epoch 4 / 5] [Batch 360 / 938] [D loss: 0.372683] [G loss: 1.621115]\n",
      "[Epoch 4 / 5] [Batch 380 / 938] [D loss: 0.127564] [G loss: 2.701440]\n",
      "[Epoch 4 / 5] [Batch 400 / 938] [D loss: 0.195360] [G loss: 1.628560]\n",
      "[Epoch 4 / 5] [Batch 420 / 938] [D loss: 0.118451] [G loss: 1.834102]\n",
      "[Epoch 4 / 5] [Batch 440 / 938] [D loss: 0.301913] [G loss: 1.789051]\n",
      "[Epoch 4 / 5] [Batch 460 / 938] [D loss: 0.292044] [G loss: 1.308674]\n",
      "[Epoch 4 / 5] [Batch 480 / 938] [D loss: 0.328887] [G loss: 1.829592]\n",
      "[Epoch 4 / 5] [Batch 500 / 938] [D loss: 0.327427] [G loss: 2.062235]\n",
      "[Epoch 4 / 5] [Batch 520 / 938] [D loss: 0.281440] [G loss: 2.260630]\n",
      "[Epoch 4 / 5] [Batch 540 / 938] [D loss: 0.224832] [G loss: 1.503891]\n",
      "[Epoch 4 / 5] [Batch 560 / 938] [D loss: 0.402584] [G loss: 2.131590]\n",
      "[Epoch 4 / 5] [Batch 580 / 938] [D loss: 1.187473] [G loss: 4.772738]\n",
      "[Epoch 4 / 5] [Batch 600 / 938] [D loss: 0.336611] [G loss: 1.122572]\n",
      "[Epoch 4 / 5] [Batch 620 / 938] [D loss: 0.251890] [G loss: 2.950652]\n",
      "[Epoch 4 / 5] [Batch 640 / 938] [D loss: 0.241883] [G loss: 2.985344]\n",
      "[Epoch 4 / 5] [Batch 660 / 938] [D loss: 0.290412] [G loss: 3.779296]\n",
      "[Epoch 4 / 5] [Batch 680 / 938] [D loss: 0.373362] [G loss: 2.035189]\n",
      "[Epoch 4 / 5] [Batch 700 / 938] [D loss: 0.238937] [G loss: 1.751696]\n",
      "[Epoch 4 / 5] [Batch 720 / 938] [D loss: 0.266528] [G loss: 1.242925]\n",
      "[Epoch 4 / 5] [Batch 740 / 938] [D loss: 0.252497] [G loss: 1.697426]\n",
      "[Epoch 4 / 5] [Batch 760 / 938] [D loss: 0.240896] [G loss: 1.522484]\n",
      "[Epoch 4 / 5] [Batch 780 / 938] [D loss: 0.296078] [G loss: 1.685659]\n",
      "[Epoch 4 / 5] [Batch 800 / 938] [D loss: 0.250994] [G loss: 1.142864]\n",
      "[Epoch 4 / 5] [Batch 820 / 938] [D loss: 0.285480] [G loss: 3.458929]\n",
      "[Epoch 4 / 5] [Batch 840 / 938] [D loss: 0.291994] [G loss: 2.508265]\n",
      "[Epoch 4 / 5] [Batch 860 / 938] [D loss: 0.283815] [G loss: 1.612107]\n",
      "[Epoch 4 / 5] [Batch 880 / 938] [D loss: 0.287901] [G loss: 3.663637]\n",
      "[Epoch 4 / 5] [Batch 900 / 938] [D loss: 0.287646] [G loss: 1.430988]\n",
      "[Epoch 4 / 5] [Batch 920 / 938] [D loss: 0.156787] [G loss: 3.167651]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        # Adversarial ground truths\n",
    "        # truths y value\n",
    "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad = False)\n",
    "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad = False)\n",
    "        \n",
    "        # Confiqure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "        \n",
    "        ###################\n",
    "        # Train Generator #\n",
    "        ###################\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # sample noise as generator input\n",
    "        # using the normal distribution\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "        \n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "        \n",
    "        # Loss measures generator's ability to fool discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        #######################\n",
    "        # Train Discriminator #\n",
    "        #######################\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        # generate images detach\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        \n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print(\"[Epoch %d / %d] [Batch %d / %d] [D loss: %f] [G loss: %f]\"\n",
    "                  % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "                 )\n",
    "        \n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow = 5, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
